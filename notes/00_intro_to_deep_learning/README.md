# Introduction to Deep Learning
Deep Learning is a field of Artificial intelligence that uses neural networks to make predictions. It uses lots of data to solve complex problems, usually handled by humans. 

## Topics
 - Why neural networks
 - Logistic regression
 - Neural Networks
 - Traning Neural Network
 - Softmax
 - One-Hot encoding
 - Dropout
 - Stochastic gradient descent
 - Vanishing Gradient Problem
 - Overfitting and underfitting

## Project(Predicting Bike-Sharing Patterns)
### Background 
Bike sharing systems are new generation of traditional bike rentals where whole process from membership, rental and return back has become automatic. Through these systems, user is able to easily rent a bike from a particular position and return back at another position. Currently, there are about over 500 bike-sharing programs around the world which is composed of over 500 thousands bicycles. Today, there exists great interest in these systems due to their important role in traffic, environmental and health issues. Apart from interesting real world applications of bike sharing systems, the characteristics of data being generated by these systems make them attractive for the research. Opposed to other transport services such as bus or subway, the duration of travel, departure and arrival position is explicitly recorded in these systems. This feature turns bike sharing system into a virtual sensor network that can be used for sensing mobility in the city. Hence, it is expected that most of important events in the city could be detected via monitoring these data.

### Data Set
Bike-sharing rental process is highly correlated to the environmental and seasonal settings. For instance, weather conditions, precipitation, day of week, season, hour of the day, etc. can affect the rental behaviors. The core data set is related to  the two-year historical log corresponding to years 2011 and 2012 from Capital Bikeshare system, Washington D.C., USA which is publicly available in http://capitalbikeshare.com/system-data. Weather information are extracted from http://www.freemeteo.com. 

### Dataset Licsense
<a href="http://dx.doi.org/10.1007/s13748-013-0040-3">[1] Fanaee-T, Hadi, and Gama, Joao, "Event labeling combining ensemble detectors and background knowledge", Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg, doi:10.1007/s13748-013-0040-3.</a>

For further information about this dataset please contact <a href="mailto:hadi.fanaee@fe.up.pt">Hadi Fanaee-T(hadi.fanaee@fe.up.pt)</a>

## Why neural networks?
Neural Networks mimics the process of how the brain operates with neurons that fires bits of information. Let's understand it using an example taken by the Udacity course. Our task, in the example, to predict whether a student is accepted or rejected in admission based on last year's data.
<p align="center"><img src="./images/admission_linear.png" height="300"/></p>

In the above image, blue ones got accepted, and red ones got rejected in college admission. First, let us try to solve the problem using linear regression than we get a straight line. But sometimes the problem is more complicated there are more than and data cannot be separated using a straight line. So now, we solve our problem using the error function. An error function is simply something that tells how badly we are doing and how far we from the solution. Our target is to decrease error value consistently to reach an optimal solution.

## Logistic Regression
<p align="center"><img src="./images/perceptron.png" height="300"/></p>
Perceptron as the same as human neurons given a set of inputs and produces an output. We use activation to function translates the output of the perceptron to the desired output. The neat thing about the above algorithm is that instead of computing the weights ourselves, they get computed by the algorithm using gradient descent.

Now let's design an algorithm to solve our problem using a perceptron. First, we'll be taking data and feeding it to a random single perceptron model. Then next, we'll be calculating the error generated by that model and further decreasing that error using gradient descent and iterate till the optimal model gets generated. But still, we get a straight line which suitable for line data, but for non-linear data, as shown in the below image, we require something more complex.

## Neural Networks
The solution to solve the non-linear model is Neural Networks. To build our neural network, we use the perceptron (Linear Model) as a foundation building block. We combine multiple linear models to create non-linear models, and then these combine to create even more non-linear models. Hence we get a deep neural network with multiple layers stacked together.
 
For more complicated problems like recognizing a digit from an image,  so problem turns from binary classification to multi-class classification.  For this, we add more nodes to the output layer. The number of the node in the output layer must be equal to the number of outputs.  

## Traning Neural Network
Now let's train a neural network. For this, we'll use the method known as backpropagation. In a nutshell, backpropagation algorithm is:
 - Doing a feedforward operation(predictions).
 - Then comparing the output of the model with the desired output and calculating the error.
 - Using backpropagation to spread the error to each of the weights. Then using this to update the weights and get a better model.
 - Continue this until we have a good model.

## Softmax
<p align="center"><img src="./images/softmax_function.png" height="100"/></p>
For multiple class classifcation, we uses softmax funtion. Technically it normalizes the outputs to a probability distribution whose total sums up to 1.
<p align="center"><img src="./images/softmax.png" height="300"/></p>

## One-Hot encoding
For categorical data where no ordinal relationship exists, then integer encoding is not enough. Because if our model internally calculates the average of categorical value, that would imply one of the value is the average of others. To avoid this, we perform perform “binarization” of categorical data as shown in the image below: 
<p align="center"><img src="./images/one_hot_encoding.png" height="300"/></p>

## Dropout
While training, some nodes can get overtrained, which can dominate our results. To avoid this problem, we drop some of the nodes randomly, and on average, each node gets the same treatment.

## Stochastic gradient descent
To avoid enormous in a single step resulting in the use of tons of memory for a single step, we use stochastic gradient descent.  To solve this, we split all the data into several random batches and train the neural network using it,  further calculates the error and its gradient and back-propagate to update the weights. Each step is less accurate than using all the data, but it's much better to take a bunch of slightly inaccurate steps than to take a good one.

## Vanishing Gradient Problem
Taking a look at the sigmoid function, we can see that when the values are very high or very low, the function is almost horizontal. That gives us an almost 0 gradient so that each step would be tiny, and we could end all the steps without arriving at the point that minimizes the error. To solve this problem, we can use another activation function like tanh or RELU function.

## Overfitting and underfitting
### Overfitting
<p align="center"><img src="./images/overfitting.png" height="300"/></p>
Overfitting is when the model learns noise in data to point when it affects its performance. Studying too much for exams and mugging up word by word and failing to understand the meaning of the text.

### Underfitting
<p align="center"><img src="./images/underfitting.png" height="300"/></p>
Underfitting is when model cannot genalize on hte traning data.Underfitting can also be viewed as not studying enough for an exam and failing a test.

### Model complexity graph
<p align="center"><img src="./images/modelComplexityGraph.png" height="300"/></p>
As long as the neural network is running, the error on the training set would be getting lower and lower. The error on the testing, on the other hand, starts to increase when the model starts to overfit. To not overfit the model, we should stop the iterations when the testing error starts to increase, and it is called early stopping.